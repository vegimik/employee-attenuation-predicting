{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a89c8eb5",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>INTRODUCTION</center></h1>\n",
    "\n",
    "- Data contain different variable that are sub classified further:\n",
    "- 1.Demographics of the Employee \n",
    "- 2.Tenure Information \n",
    "- 3.Historical data regarding performance\n",
    "- Using these data we will going to find out whether employee will stay in the company and also will see another pattern with respect to time period of person stay in the company. There are other aspects also see in the notebook you can check in visualization part.\n",
    "- Using this data HR department will find out easily what type of candidate they need to take for particular job role.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c239c",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>LIBRARIES</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bcea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pandas import DataFrame\n",
    "import seaborn as sns\n",
    "\n",
    "#conda install -c glemaitre imbalanced-learn\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "\n",
    "# Imputing missing values and scaling values\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('dataset'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes = True)\n",
    "%matplotlib inline     \n",
    "\n",
    "\n",
    "# Machine Learning Helper Functions\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pprint import pprint\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159622b2",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>IMPORT_DATA</center></h1>\n",
    "\n",
    "# 2.Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1daad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset\n",
    "train = pd.read_csv('dataset/train_data.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cda77a",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>DATA_PREPROCESSING</center></h1>\n",
    "\n",
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00377de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563eb399",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Target']=np.where(train['LastWorkingDate'].isnull(),0,1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cbf7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_groupby = train.groupby(['Emp_ID'])['Emp_ID','Age','Gender','City','Education_Level','Salary','Joining Designation','Designation',\n",
    "                       'Quarterly Rating','Target'].tail(1)\n",
    "employees_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_groupby.reset_index(inplace = True,drop = True)\n",
    "employees_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2137737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_business_groupby = train.groupby('Emp_ID').agg({'Total Business Value':'sum'})\n",
    "total_business_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_business_groupby.reset_index(drop = True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931688f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([total_business_groupby,employees_groupby],axis = 1,join = 'inner')\n",
    "final\n",
    "\n",
    "corr = final.corr()\n",
    "\n",
    "plt.figure(figsize = (20,8))\n",
    "sns.heatmap(corr,annot = True,cmap = 'rocket')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a45321",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>DATA_VISUALIZATION</center></h1>\n",
    "\n",
    "\n",
    "# 4.Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fb670",
   "metadata": {},
   "outputs": [],
   "source": [
    "catplot=sns.catplot(x='Gender',y='Total Business Value',hue = 'Education_Level',data = final,col = 'Education_Level',col_wrap = 2)\n",
    "catplot.set(xlabel='Gender',ylabel='Total Business Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e271f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot('Education_Level','Salary',data = final,hue = 'Gender',col = 'City',col_wrap = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d9bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,10))\n",
    "\n",
    "plt.subplot(231)\n",
    "count_plot=sns.countplot(x='Gender',data = final)\n",
    "count_plot.set(xlabel='Gender',ylabel='')\n",
    "\n",
    "plt.subplot(232)\n",
    "sns.countplot(x='Education_Level',data = final)\n",
    "count_plot.set(xlabel='Education_Level',ylabel='')\n",
    "\n",
    "plt.subplot(233)\n",
    "sns.countplot(x='Joining Designation',data = final)\n",
    "count_plot.set(xlabel='Joining Designation',ylabel='')\n",
    "\n",
    "plt.subplot(234)\n",
    "sns.countplot(x='Designation',data = final)\n",
    "count_plot.set(xlabel='Designation',ylabel='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d43059",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,8))\n",
    "count_plot=sns.countplot(x='City',data = final,palette= 'rocket')\n",
    "count_plot.set(xlabel='City',ylabel='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_plot=sns.countplot(x='Target',data = final)\n",
    "count_plot.set(xlabel='Target',ylabel='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61045ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['Quarterly Rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df45e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "final ['Gender'] = pd.get_dummies(final['Gender'],drop_first = True)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a3a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label = LabelEncoder()\n",
    "\n",
    "final['City'] = label.fit_transform(final['City'])\n",
    "\n",
    "final['Education_Level'] = label.fit_transform(final['Education_Level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c543f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6cce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot=sns.boxplot(x='Total Business Value',data = final)\n",
    "box_plot.set(xlabel='Total Business Value',ylabel='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32901683",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = final['Total Business Value'].quantile(0.25)\n",
    "Q3 = final['Total Business Value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "filter = (final['Total Business Value'] >= Q1 - 1.5 * IQR) & (final['Total Business Value']<= Q3 + 1.5 *IQR)\n",
    "train1 = final.loc[filter]  \n",
    "print(\"data loss percentage {}%\".format(((len(final) - len(train1))/len(final))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7738179",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot=sns.boxplot(x='Salary',data=final)\n",
    "box_plot.set(xlabel='Salary',ylabel='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836005d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = final['Salary'].quantile(0.25)\n",
    "Q3 = final['Salary'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "filter = (final['Salary'] >= Q1 - 1.5 * IQR) & (final['Salary']<= Q3 + 1.5 *IQR)\n",
    "train2 = final.loc[filter]  \n",
    "print(\"data loss percentage {}%\".format(((len(final) - len(train2))/len(final))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final.to_csv('dataset/employee-attenuation-predicting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632a183",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b253b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the Fraud and the normal dataset \n",
    "\n",
    "not_stay= (train2['Target']== 0 )\n",
    "\n",
    "stay= (train2['Target']== 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "std_x = train2.loc[:,['Total Business Value','Age','Gender','City','Education_Level','Salary','Joining Designation','Designation','Quarterly Rating']]\n",
    "std_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0da6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train2.iloc[:,-1]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bfa1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Undersampling for Handling Imbalanced \n",
    "nm = NearMiss()\n",
    "X_res,y_res=nm.fit_resample(std_x,y)\n",
    "\n",
    "X_res.shape,y_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59181333",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(final['Age'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9b4e46",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "<h3 style='background:#92b0ac; border:0; color:black; padding-top:15px'><center>Training and Testing set</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into Train and test format\n",
    "x_train,x_test,y_train,y_test = train_test_split(X_res,y_res,test_size = 0.25,random_state =105)\n",
    "\n",
    "print('Shape of Training Xs:{}'.format(x_train.shape))\n",
    "print('shape of Test Xt:{}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e095e4b",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black; padding-top:15px'><center>MODEL BUILDING</center></h1>\n",
    "\n",
    "\n",
    "In this section we will build and train several machine learning methods for our supervised task. The objective is to determine which model holds the most promise for further development (such as hyperparameter tuning).\n",
    "\n",
    "We are comparing models using the mean absolute error. A baseline model that guessed the median value of the score was off by an average of 25 points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2428e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imputer object with a median filling strategy\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Train on the training features\n",
    "imputer.fit(x_train)\n",
    "\n",
    "# Transform both training data and testing data\n",
    "X = imputer.transform(x_train)\n",
    "X_test = imputer.transform(x_test)\n",
    "\n",
    "print('Missing values in training features: ', np.sum(np.isnan(X)))\n",
    "print('Missing values in testing features:  ', np.sum(np.isnan(X_test)))\n",
    "\n",
    "# Make sure all values are finite\n",
    "print(np.where(~np.isfinite(X)))\n",
    "print(np.where(~np.isfinite(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7edb8",
   "metadata": {},
   "source": [
    "## 6.1. Scaling Features\n",
    "\n",
    "The final step before we can start building our models is to scale the features. This is necessary because features are in different units, and we want to normalize the features so the units do not affect the algorithm. Eventhough, Linear Regression and Random Forest do not require feature scaling, but other methods, such as support vector machines (SVM) and k nearest neighbors (KNN), do require it because they take into account the Euclidean distance between observations.\n",
    "\n",
    "There are two ways to scale features:\n",
    "<ul>\n",
    "<li>\n",
    "For each value, subtract the mean of the feature and divide by the standard deviation of the feature. This is known as standardization and results in each feature having a mean of 0 and a standard deviation of 1.\n",
    "</li>\n",
    "<li>\n",
    "For each value, subtract the minimum value of the feature and divide by the maximum minus the minimum for the feature (the range). This assures that all the values for a feature are between 0 and 1 and is called scaling to a range or normalization.\n",
    "</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c689ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Features\n",
    "\n",
    "# Create the scaler object with a range of 0-1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit on the training data\n",
    "scaler.fit(X)\n",
    "\n",
    "# Transform both the training and testing data\n",
    "X = scaler.transform(X)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert y to one-dimensional array (vector)\n",
    "y = np.array(y_train).reshape((-1, ))\n",
    "y_test = np.array(y_test).reshape((-1, ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad87ba1",
   "metadata": {},
   "source": [
    "\n",
    "### Models throught algorithms\n",
    "\n",
    "We will compare five different machine learning models using the great Scikit-Learn library:\n",
    "\n",
    "There are several ways for modeling:\n",
    "<ul>\n",
    "<li>\n",
    "Logistic Regression\n",
    "</li>\n",
    "<li>\n",
    "Support Vector Machine \n",
    "</li>\n",
    "<li>\n",
    "Random Forest Classifier\n",
    "</li>\n",
    "<li>\n",
    "Gradient Boosting Classifier\n",
    "    \n",
    "</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for saving results\n",
    "accuracy_result = []\n",
    "models_result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate mean absolute error\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(abs(y_true - y_pred))\n",
    "\n",
    "# Takes in a model, trains the model, and evaluates the model on the test set\n",
    "def fit_and_evaluate(model):\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Make predictions and evalute\n",
    "    model_pred = model.predict(X_test)\n",
    "    model_mae = mae(y_test, model_pred)\n",
    "    \n",
    "    accuracy_result.append(model_mae*100)\n",
    "    \n",
    "    # Return the performance metric\n",
    "    #return model_mae\n",
    "    return {\"model_mae\": model_mae, \"accuracy\": model_mae*100, \"model_pred\": model_pred}\n",
    "\n",
    "def fit_and_evaluate_model(model):\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Make predictions and evalute\n",
    "    model_pred = model.predict(X_test)\n",
    "    model_mae = mae(y_test, model_pred)\n",
    "    \n",
    "    model_accuracy = accuracy_score(y_test, model_pred)\n",
    "    model_accuracy = str(model_accuracy*100) + ' %'\n",
    "    \n",
    "    accuracy_result.append(model_accuracy)\n",
    "    \n",
    "    # Return the performance metric\n",
    "    return {\"mae\": str(model_mae), \"accuracy\": model_accuracy, \"model_pred\": model_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b58bd6",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772febec",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_result.append('Logistic Regression')\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg_model = fit_and_evaluate_model(log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_pred=log_reg_model['model_pred']\n",
    "print('Mean Absolute Error: ' + str(log_reg_model['mae']))\n",
    "print('Accuracy: ' + str(log_reg_model['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lg_res = X_res['Total Business Value'].values.reshape(-1,1)\n",
    "y_lg_res = y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45123c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(x_lg_res,y_lg_res)\n",
    "y_lr_pred = lr_model.predict(x_lg_res)\n",
    "\n",
    "print(y_lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b0e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_lg_res,y_lg_res)\n",
    "plt.xlabel('Total Business Value')\n",
    "plt.ylabel('Target')\n",
    "plt.plot(x_lg_res,y_lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = lr_model.intercept_\n",
    "theta1 = lr_model.coef_\n",
    "theta0, theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lr_pred = lr_model.predict(np.array([80900]).reshape(1,1))\n",
    "y_lr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725db271",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lr_pred = lr_model.predict(np.array([49957]).reshape(1,1))\n",
    "y_lr_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f5b887",
   "metadata": {},
   "source": [
    "### Multivariant logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6411fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "attenuation_features = ['Education_Level','Total Business Value','Quarterly Rating']\n",
    "x_lg_res = X_res[attenuation_features]\n",
    "y_lg_res = y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962290a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,2,1)\n",
    "plt.scatter(X_res['Education_Level'],y_lg_res)\n",
    "plt.subplot(2,2,2)\n",
    "plt.scatter(X_res['Total Business Value'],y_lg_res)\n",
    "plt.subplot(2,2,3)\n",
    "plt.scatter(X_res['Quarterly Rating'],y_lg_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ce0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,projection='3d')\n",
    "\n",
    "x1_lg_res = X_res['Education_Level']\n",
    "x2_lg_res = X_res['Total Business Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d0f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.scatter(x1_lg_res,x2_lg_res,y_lg_res, c='r', marker = 'o')\n",
    "ax.set_xlabel('Education Level')\n",
    "ax.set_ylabel('Total Business Value')\n",
    "ax.set_zlabel('Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91658b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.scatter(x1_lg_res,x2_lg_res,y_lg_res, c='r', marker = 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c795d6",
   "metadata": {},
   "source": [
    "#### Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_result.append('Support Vector Classification')\n",
    "\n",
    "svm = SVC(C = 1000, gamma = 0.1)\n",
    "svm_model = fit_and_evaluate_model(svm)\n",
    "\n",
    "print('Mean Absolute Error: ' + str(svm_model['mae']))\n",
    "print('Accuracy: ' + str(svm_model['accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c927b5",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7697b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function which will give us train and test accuracy for each classifier.\n",
    "def train_test_error(y_train_data,y_test_data):\n",
    "    train_error = ((y_train_data==y).sum())/len(y_train_data)*100\n",
    "    test_error = ((y_test_data==y_test).sum())/len(y_test)*100\n",
    "    accuracy_result.append(test_error)\n",
    "    print('Decision Tree Performance: {}'.format(train_error) + \" is the train accuracy\")\n",
    "    print('Decision Tree Performance: {}'.format(test_error) + \" is the test accuracy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_result.append('Decision Tree')\n",
    "\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "dtc_model = fit_and_evaluate_model(dtc)\n",
    "\n",
    "print('Mean Absolute Error: ' + str(dtc_model['mae']))\n",
    "print('Accuracy: ' + str(dtc_model['accuracy']))\n",
    "#dtc.fit(X,y)\n",
    "#train_predict = dtc.predict(X)\n",
    "#test_predict = dtc.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e0f68",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98779179",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_result.append('Random Forest Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670341a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "rfc_metrics = fit_and_evaluate_model(rfc);\n",
    "print('Mean Absolute Error: ' + str(rfc_metrics['mae']))\n",
    "print('Accuracy: ' + str(rfc_metrics['accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000fcde7",
   "metadata": {},
   "source": [
    "## All results from algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8359999",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = DataFrame({\"Accuracy\" : accuracy_result } , index = models_result)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ace3ce",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>MODEL_REBUILDING(ANALYZING_AND_EVALUATION)</center></h1>\n",
    "\n",
    "# 7. Rebuilding model ( analyzing and evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65639298",
   "metadata": {},
   "source": [
    "###### Model Optimization\n",
    "In machine learning, optimizing a model means finding the best set of hyperparameters for a particular problem.\n",
    "\n",
    "###### Hyperparameters\n",
    "Model hyperparameters are best thought of as settings for a machine learning algorithm that are tuned by the data scientist before training. Examples would be the number of trees in the random forest, or the number of neighbors used in K Nearest Neighbors Regression.\n",
    "Model parameters are what the model learns during training, such as the weights in the linear regression.\n",
    "We as data scientists control a model by choosing the hyperparameters, and these choices can have a significant effect on the final performance of the model (although usually not as great of an effect as getting more data or engineering features).\n",
    "Tuning the model hyperparameters controls the balance of under vs over fitting in a model. We can try to correct for under-fitting by making a more complex model, such as using more trees in a random forest or more layers in a deep neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be169da",
   "metadata": {},
   "source": [
    "### Random Forest - Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb9263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rfc.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b8c93",
   "metadata": {},
   "source": [
    "###### Random Hyperparameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64662010",
   "metadata": {},
   "source": [
    "###### Random Search Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc58a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_rand = RandomForestClassifier();\n",
    "rf_random = RandomizedSearchCV(estimator = rf_rand, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=0, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b935300",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a038b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rfc(model):\n",
    "   # Train the model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Make predictions and evalute\n",
    "    model_pred = model.predict(X_test)\n",
    "    model_mae = mae(y_test, model_pred)\n",
    "    \n",
    "    model_accuracy = accuracy_score(y_test, model_pred)\n",
    "    model_accuracy = str(model_accuracy*100) + ' %'\n",
    "    \n",
    "    accuracy_result.append(model_accuracy)\n",
    "    \n",
    "    # Return the performance metric\n",
    "    return {\"mae\": str(model_mae), \"accuracy\": model_accuracy, \"model_pred\": model_pred}\n",
    "\n",
    "print(rfc_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdaee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_random = rf_random.best_estimator_\n",
    "#random_accuracy = evaluate_rfc(best_random)\n",
    "#print(random_accuracy)\n",
    "#Commented out: BECAUSE IT IS SLOWING DOWN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed03db6",
   "metadata": {},
   "source": [
    "###### Grid Search with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c784f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_search.fit(X, y)\n",
    "#grid_search.best_params_\n",
    "\n",
    "#best_grid = grid_search.best_estimator_\n",
    "#grid_accuracy = evaluate_rfc(best_grid)\n",
    "#print(grid_accuracy) \n",
    "#Commented out: BECAUSE IT IS SLOWING DOWN //"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdfbe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = list(final.columns)\n",
    "# Get numerical feature importances\n",
    "importances = list(rfc.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3976e9",
   "metadata": {},
   "source": [
    "### Logisitic Regression - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351c964",
   "metadata": {},
   "source": [
    "#### Hyperparameter using StandardScaler and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3dd147",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_slc = StandardScaler()\n",
    "pca = decomposition.PCA()\n",
    "logistic_Reg = lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline will helps us by passing modules one by one through GridSearchCV for which we want to get the best parameters.\n",
    "pipe = Pipeline(steps=[('std_slc', std_slc),\n",
    "                       ('pca', pca),\n",
    "                       ('logistic_Reg', logistic_Reg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e30355",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = list(range(1,X.shape[1]+1,1))\n",
    "C = np.logspace(-4, 4, 50)\n",
    "penalty = ['l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(pca__n_components=n_components,\n",
    "                  logistic_Reg__C=C,\n",
    "                  logistic_Reg__penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffcd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_GS = GridSearchCV(pipe, parameters)\n",
    "clf_GS.fit(X, y)\n",
    "\n",
    "print('Best Penalty:', clf_GS.best_estimator_.get_params()['logistic_Reg__penalty'])\n",
    "print('Best C:', clf_GS.best_estimator_.get_params()['logistic_Reg__C'])\n",
    "print('Best Number Of Components:', clf_GS.best_estimator_.get_params()['pca__n_components'])\n",
    "print(); \n",
    "print(clf_GS.best_estimator_.get_params()['logistic_Reg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84edf4f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ded2fa22",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb6bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    X,\n",
    "    y,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "    cv=None,\n",
    "    n_jobs=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "):\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        return_times=True,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "title = \"Learning Curves (GaussianNB)\"\n",
    "# Cross validation with 50 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = GaussianNB()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "title = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "estimator = svm# svm type of SVC(C = 1000, gamma = 0.1)\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "title = r\"Learning Curves (Random Forest )\"\n",
    "plot_learning_curve(\n",
    "    rfc, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89a9b8",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>CONFUSION_MATRIX</center></h1>\n",
    "\n",
    "\n",
    "# 8. Confusion Matirx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb29139",
   "metadata": {},
   "source": [
    "### Compute confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, log_reg_model_pred)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "cnf_matrix\n",
    "\n",
    "print(classification_report(y_test,log_reg_model_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b2677",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>CONCLUSION</center></h1>\n",
    "\n",
    "\n",
    "# 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468845c9",
   "metadata": {},
   "source": [
    "### Logisitc Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d9449c",
   "metadata": {},
   "source": [
    "Hypothesis: h<sub>Θ</sub>(x)=g(Θ<sup>T</sup>*x) , h<sub>Θ</sub>(x)=g(Θ<sub>0</sub>*x<sub>0</sub>+ Θ<sub>1</sub>*x<sub>1</sub>+ ... + Θ<sub>n</sub>*x<sub>n</sub>) , g(z)= 1/(1- e<sup>-z</sup>)\n",
    "<br/> \n",
    "=> h<sub>Θ</sub>(x)=1/(1- e<sup>Θ<sub>0</sub>*x<sub>0</sub>+ Θ<sub>1</sub>*x<sub>1</sub>+ ... + Θ<sub>n</sub>*x<sub>n</sub></sup>)\n",
    "\n",
    "Suppose predict “y=1\" for h<sub>Θ</sub>(x)>=1/2 or \"y=0\" otherwise\n",
    "\n",
    "Logisitc regression cost function: J(Θ) = 1/(m) Σ {Cost (h<sub>Θ</sub>(x), y<sup>i</sup>)}=-1/m Σ{y<sup>i</sup>*log(h<sub>Θ</sub>(x))+(1-y<sup>i</sup>)log(1- h<sub>Θ</sub>(x))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1469c1",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we covered crucial concepts in the machine learning for dataset \"Employee Attenuation Predicting\":\n",
    "\n",
    "<ul>\n",
    "<li>Imputing missing values\n",
    "</li>\n",
    "<li>Data proprocessing\n",
    "</li>\n",
    "<li>Data vizualization\n",
    "</li>\n",
    "<li>Builing model\n",
    "    <ol>\n",
    "    <li>Scaling Features\n",
    "    </li>\n",
    "    <li>Models through Algorithms\n",
    "        <ul>\n",
    "        <li>Logisitc Regression\n",
    "        </li>\n",
    "        <li>SVM\n",
    "        </li>\n",
    "        <li>Random Forest\n",
    "        </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    </ol>\n",
    "</li>Evaluating and comparing several machine learning methods\n",
    "<li>Hyperparameter tuning a machine learning model\n",
    "</li>\n",
    "<li>Learning Curves\n",
    "</li>\n",
    "<li>Confusion Matrix\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "The results showed us that machine learning is applicable to our problem (find out whether employee will stay in the company and also will see another pattern with respect to time period of person stay in the company). \n",
    "Moreover, find out easily what type of candidate they need to take for particular job role, with the final model able to the predict the Employee Attenuation Predicting. We also saw that hyperparamter tuning was able to improve the performance of the model although at a considerable cost in terms of <i>'Education_Level','Total Business Value','Quarterly Rating'</i> especially to Logisitc Regression. This is a good reminder that proper feature engineering and gathering more data (if possible!) has a much larger pay-off than fine-tuning the model. From the learning curves we have seen there is neither built overfitting model nor  underfitting in the full term of it but based on the multivariants visualization we can come up with higher accuracy if we get a complex function but not only based in the idea of polynomial fuction but in the additional atribbute connection to Target attribute. We also observed the trade-off in run-time versus accuracy, which is one of many considerations we have to take into account when designing machine learning models.\n",
    "It is possible to use this model for feature selection and implement a simpler model that is more interpretable!\n",
    "In terms of accuracy random forest performs better,\n",
    "with a small difference after the hyperparameter tuning process.\n",
    "According to the model we see that the three most important attributes in the prediction in the random forest algorithm are:\n",
    "  Total Business Value (28%),\n",
    "  Designation (20%)\n",
    "  Education_Level (18%).\n",
    "But it is worth noting that the random forest algorithm requires more resources, such as time, because it performs a large number of iterations. Also to make a comparison was used the Weka tool with which the model was created using the random forest algorithm from which we derive results similar to those developed in Jupiter notebook.\n",
    "\n",
    "In the final notebook, we will try to answer these questions and draw final conclusions from the project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
