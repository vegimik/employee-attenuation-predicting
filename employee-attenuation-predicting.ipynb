{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a89c8eb5",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>INTRODUCTION</center></h1>\n",
    "\n",
    "- Data contain different variable that are sub classified further:\n",
    "- 1.Demographics of the Employee \n",
    "- 2.Tenure Information \n",
    "- 3.Historical data regarding performance\n",
    "- Using these data we will going to find out whether employee will stay in the company and also will see another pattern with respect to time period of person stay in the company. There are other aspects also see in the notebook you can check in visualization part.\n",
    "- Using this data HR department will find out easily what type of candidate they need to take for particular job role.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c239c",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>LIBRARIES</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bcea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pandas import DataFrame\n",
    "import seaborn as sns\n",
    "\n",
    "#conda install -c glemaitre imbalanced-learn\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "\n",
    "# Imputing missing values and scaling values\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('dataset'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes = True)\n",
    "%matplotlib inline     \n",
    "\n",
    "\n",
    "# Machine Learning Helper Functions\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pprint import pprint\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159622b2",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>IMPORT_DATA</center></h1>\n",
    "\n",
    "# 2.Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1daad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset\n",
    "train = pd.read_csv('dataset/train_data.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cda77a",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>DATA_PREPROCESSING</center></h1>\n",
    "\n",
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00377de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563eb399",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Target']=np.where(train['LastWorkingDate'].isnull(),0,1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cbf7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_groupby = train.groupby(['Emp_ID'])['Emp_ID','Age','Gender','City','Education_Level','Salary','Joining Designation','Designation',\n",
    "                       'Quarterly Rating','Target'].tail(1)\n",
    "employees_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_groupby.reset_index(inplace = True,drop = True)\n",
    "employees_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2137737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_business_groupby = train.groupby('Emp_ID').agg({'Total Business Value':'sum'})\n",
    "total_business_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_business_groupby.reset_index(drop = True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931688f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([total_business_groupby,employees_groupby],axis = 1,join = 'inner')\n",
    "final\n",
    "\n",
    "corr = final.corr()\n",
    "\n",
    "plt.figure(figsize = (20,8))\n",
    "sns.heatmap(corr,annot = True,cmap = 'rocket')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a45321",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>DATA_VISUALIZATION</center></h1>\n",
    "\n",
    "\n",
    "# 4.Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fb670",
   "metadata": {},
   "outputs": [],
   "source": [
    "catplot=sns.catplot(x='Gender',y='Total Business Value',hue = 'Education_Level',data = final,col = 'Education_Level',col_wrap = 2)\n",
    "catplot.set(xlabel='Gender',ylabel='Total Business Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e271f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot('Education_Level','Salary',data = final,hue = 'Gender',col = 'City',col_wrap = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d9bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,10))\n",
    "\n",
    "plt.subplot(231)\n",
    "count_plot=sns.countplot(x='Gender',data = final)\n",
    "count_plot.set(xlabel='Gender',ylabel='')\n",
    "\n",
    "plt.subplot(232)\n",
    "sns.countplot(x='Education_Level',data = final)\n",
    "count_plot.set(xlabel='Education_Level',ylabel='')\n",
    "\n",
    "plt.subplot(233)\n",
    "sns.countplot(x='Joining Designation',data = final)\n",
    "count_plot.set(xlabel='Joining Designation',ylabel='')\n",
    "\n",
    "plt.subplot(234)\n",
    "sns.countplot(x='Designation',data = final)\n",
    "count_plot.set(xlabel='Designation',ylabel='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d43059",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,8))\n",
    "count_plot=sns.countplot(x='City',data = final,palette= 'rocket')\n",
    "count_plot.set(xlabel='City',ylabel='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_plot=sns.countplot(x='Target',data = final)\n",
    "count_plot.set(xlabel='Target',ylabel='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61045ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['Quarterly Rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df45e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "final ['Gender'] = pd.get_dummies(final['Gender'],drop_first = True)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a3a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label = LabelEncoder()\n",
    "\n",
    "final['City'] = label.fit_transform(final['City'])\n",
    "\n",
    "final['Education_Level'] = label.fit_transform(final['Education_Level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c543f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6cce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot=sns.boxplot(x='Total Business Value',data = final)\n",
    "box_plot.set(xlabel='Total Business Value',ylabel='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32901683",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = final['Total Business Value'].quantile(0.25)\n",
    "Q3 = final['Total Business Value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "filter = (final['Total Business Value'] >= Q1 - 1.5 * IQR) & (final['Total Business Value']<= Q3 + 1.5 *IQR)\n",
    "train1 = final.loc[filter]  \n",
    "print(\"data loss percentage {}%\".format(((len(final) - len(train1))/len(final))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7738179",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot=sns.boxplot(x='Salary',data=final)\n",
    "box_plot.set(xlabel='Salary',ylabel='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836005d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = final['Salary'].quantile(0.25)\n",
    "Q3 = final['Salary'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "filter = (final['Salary'] >= Q1 - 1.5 * IQR) & (final['Salary']<= Q3 + 1.5 *IQR)\n",
    "train2 = final.loc[filter]  \n",
    "print(\"data loss percentage {}%\".format(((len(final) - len(train2))/len(final))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('dataset/employee-attenuation-predicting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632a183",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b253b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the Fraud and the normal dataset \n",
    "\n",
    "not_stay= (train2['Target']== 0 )\n",
    "\n",
    "stay= (train2['Target']== 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "std_x = train2.loc[:,['Total Business Value','Age','Gender','City','Education_Level','Salary','Joining Designation','Designation','Quarterly Rating']]\n",
    "std_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0da6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train2.iloc[:,-1]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bfa1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Undersampling for Handling Imbalanced \n",
    "nm = NearMiss()\n",
    "X_res,y_res=nm.fit_resample(std_x,y)\n",
    "\n",
    "X_res.shape,y_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59181333",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(final['Age'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9b4e46",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "<h3 style='background:#92b0ac; border:0; color:black; padding-top:15px'><center>Training and Testing set</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into Train and test format\n",
    "x_train,x_test,y_train,y_test = train_test_split(X_res,y_res,test_size = 0.25,random_state =105)\n",
    "\n",
    "print('Shape of Training Xs:{}'.format(x_train.shape))\n",
    "print('shape of Test Xt:{}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e095e4b",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black; padding-top:15px'><center>MODEL BUILDING</center></h1>\n",
    "\n",
    "\n",
    "In this section we will build and train several machine learning methods for our supervised task. The objective is to determine which model holds the most promise for further development (such as hyperparameter tuning).\n",
    "\n",
    "We are comparing models using the mean absolute error. A baseline model that guessed the median value of the score was off by an average of 25 points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2428e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imputer object with a median filling strategy\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Train on the training features\n",
    "imputer.fit(x_train)\n",
    "\n",
    "# Transform both training data and testing data\n",
    "X = imputer.transform(x_train)\n",
    "X_test = imputer.transform(x_test)\n",
    "\n",
    "print('Missing values in training features: ', np.sum(np.isnan(X)))\n",
    "print('Missing values in testing features:  ', np.sum(np.isnan(X_test)))\n",
    "\n",
    "# Make sure all values are finite\n",
    "print(np.where(~np.isfinite(X)))\n",
    "print(np.where(~np.isfinite(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7edb8",
   "metadata": {},
   "source": [
    "## 6.1. Scaling Features\n",
    "\n",
    "The final step before we can start building our models is to scale the features. This is necessary because features are in different units, and we want to normalize the features so the units do not affect the algorithm. Eventhough, Linear Regression and Random Forest do not require feature scaling, but other methods, such as support vector machines (SVM) and k nearest neighbors (KNN), do require it because they take into account the Euclidean distance between observations.\n",
    "\n",
    "There are two ways to scale features:\n",
    "<ul>\n",
    "<li>\n",
    "For each value, subtract the mean of the feature and divide by the standard deviation of the feature. This is known as standardization and results in each feature having a mean of 0 and a standard deviation of 1.\n",
    "</li>\n",
    "<li>\n",
    "For each value, subtract the minimum value of the feature and divide by the maximum minus the minimum for the feature (the range). This assures that all the values for a feature are between 0 and 1 and is called scaling to a range or normalization.\n",
    "</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c689ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Features\n",
    "\n",
    "# Create the scaler object with a range of 0-1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit on the training data\n",
    "scaler.fit(X)\n",
    "\n",
    "# Transform both the training and testing data\n",
    "X = scaler.transform(X)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert y to one-dimensional array (vector)\n",
    "y = np.array(y_train).reshape((-1, ))\n",
    "y_test = np.array(y_test).reshape((-1, ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad87ba1",
   "metadata": {},
   "source": [
    "\n",
    "### Models throught algorithms\n",
    "\n",
    "We will compare five different machine learning models using the great Scikit-Learn library:\n",
    "\n",
    "There are several ways for modeling:\n",
    "<ul>\n",
    "<li>\n",
    "Logistic Regression\n",
    "</li>\n",
    "<li>\n",
    "Support Vector Machine \n",
    "</li>\n",
    "<li>\n",
    "Random Forest Classifier\n",
    "</li>\n",
    "<li>\n",
    "Gradient Boosting Classifier\n",
    "    \n",
    "</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for saving results\n",
    "accuracy_result = []\n",
    "models_result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate mean absolute error\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(abs(y_true - y_pred))\n",
    "\n",
    "# Takes in a model, trains the model, and evaluates the model on the test set\n",
    "def fit_and_evaluate(model):\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Make predictions and evalute\n",
    "    model_pred = model.predict(X_test)\n",
    "    model_mae = mae(y_test, model_pred)\n",
    "    \n",
    "    accuracy_result.append(model_mae*100)\n",
    "    \n",
    "    # Return the performance metric\n",
    "    #return model_mae\n",
    "    return {\"model_mae\": model_mae, \"accuracy\": model_mae*100, \"model_pred\": model_pred}\n",
    "\n",
    "def fit_and_evaluate_model(model):\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Make predictions and evalute\n",
    "    model_pred = model.predict(X_test)\n",
    "    model_mae = mae(y_test, model_pred)\n",
    "    \n",
    "    model_accuracy = accuracy_score(y_test, model_pred)\n",
    "    model_accuracy = str(model_accuracy*100) + ' %'\n",
    "    \n",
    "    accuracy_result.append(model_accuracy)\n",
    "    \n",
    "    # Return the performance metric\n",
    "    return {\"mae\": str(model_mae), \"accuracy\": model_accuracy, \"model_pred\": model_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b58bd6",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772febec",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_result.append('Logistic Regression')\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg_model = fit_and_evaluate_model(log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_pred=log_reg_model['model_pred']\n",
    "print('Mean Absolute Error: ' + str(log_reg_model['mae']))\n",
    "print('Accuracy: ' + str(log_reg_model['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lg_res = X_res['Total Business Value'].values.reshape(-1,1)\n",
    "y_lg_res = y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45123c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(x_lg_res,y_lg_res)\n",
    "y_lr_pred = lr_model.predict(x_lg_res)\n",
    "\n",
    "print(y_lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b0e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_lg_res,y_lg_res)\n",
    "plt.xlabel('Total Business Value')\n",
    "plt.ylabel('Target')\n",
    "plt.plot(x_lg_res,y_lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = lr_model.intercept_\n",
    "theta1 = lr_model.coef_\n",
    "theta0, theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lr_pred = lr_model.predict(np.array([80900]).reshape(1,1))\n",
    "y_lr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725db271",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lr_pred = lr_model.predict(np.array([49957]).reshape(1,1))\n",
    "y_lr_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f5b887",
   "metadata": {},
   "source": [
    "### Multivariant logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6411fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "attenuation_features = ['Education_Level','Total Business Value','Quarterly Rating']\n",
    "x_lg_res = X_res[attenuation_features]\n",
    "y_lg_res = y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962290a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,2,1)\n",
    "plt.scatter(X_res['Education_Level'],y_lg_res)\n",
    "plt.subplot(2,2,2)\n",
    "plt.scatter(X_res['Total Business Value'],y_lg_res)\n",
    "plt.subplot(2,2,3)\n",
    "plt.scatter(X_res['Quarterly Rating'],y_lg_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ce0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,projection='3d')\n",
    "\n",
    "x1_lg_res = X_res['Education_Level']\n",
    "x2_lg_res = X_res['Total Business Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d0f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.scatter(x1_lg_res,x2_lg_res,y_lg_res, c='r', marker = 'o')\n",
    "ax.set_xlabel('Education Level')\n",
    "ax.set_ylabel('Total Business Value')\n",
    "ax.set_zlabel('Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91658b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.scatter(x1_lg_res,x2_lg_res,y_lg_res, c='r', marker = 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c795d6",
   "metadata": {},
   "source": [
    "#### Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_result.append('Support Vector Classification')\n",
    "\n",
    "svm = SVC(C = 1000, gamma = 0.1)\n",
    "svm_model = fit_and_evaluate_model(svm)\n",
    "\n",
    "print('Mean Absolute Error: ' + str(svm_model['mae']))\n",
    "print('Accuracy: ' + str(svm_model['accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c927b5",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7697b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function which will give us train and test accuracy for each classifier.\n",
    "def train_test_error(y_train_data,y_test_data):\n",
    "    train_error = ((y_train_data==y).sum())/len(y_train_data)*100\n",
    "    test_error = ((y_test_data==y_test).sum())/len(y_test)*100\n",
    "    accuracy_result.append(test_error)\n",
    "    print('Decision Tree Performance: {}'.format(train_error) + \" is the train accuracy\")\n",
    "    print('Decision Tree Performance: {}'.format(test_error) + \" is the test accuracy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_result.append('Decision Tree')\n",
    "\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "dtc_model = fit_and_evaluate_model(dtc)\n",
    "\n",
    "print('Mean Absolute Error: ' + str(dtc_model['mae']))\n",
    "print('Accuracy: ' + str(dtc_model['accuracy']))\n",
    "#dtc.fit(X,y)\n",
    "#train_predict = dtc.predict(X)\n",
    "#test_predict = dtc.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e0f68",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96885398",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_result.append('Random Forest Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670341a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "rfc_metrics = fit_and_evaluate_model(rfc);\n",
    "print('Mean Absolute Error: ' + str(rfc_metrics['mae']))\n",
    "print('Accuracy: ' + str(rfc_metrics['accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000fcde7",
   "metadata": {},
   "source": [
    "## All results from algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8359999",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = DataFrame({\"Accuracy\" : accuracy_result } , index = models_result)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f7f1e",
   "metadata": {},
   "source": [
    "### Random Forest - Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb9263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rfc.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b8c93",
   "metadata": {},
   "source": [
    "###### Random Hyperparameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64662010",
   "metadata": {},
   "source": [
    "###### Random Search Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc58a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_rand = RandomForestClassifier();\n",
    "rf_random = RandomizedSearchCV(estimator = rf_rand, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=0, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b935300",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a038b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rfc(model):\n",
    "   # Train the model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Make predictions and evalute\n",
    "    model_pred = model.predict(X_test)\n",
    "    model_mae = mae(y_test, model_pred)\n",
    "    \n",
    "    model_accuracy = accuracy_score(y_test, model_pred)\n",
    "    model_accuracy = str(model_accuracy*100) + ' %'\n",
    "    \n",
    "    accuracy_result.append(model_accuracy)\n",
    "    \n",
    "    # Return the performance metric\n",
    "    return {\"mae\": str(model_mae), \"accuracy\": model_accuracy, \"model_pred\": model_pred}\n",
    "\n",
    "print(rfc_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdaee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_random = rf_random.best_estimator_\n",
    "#random_accuracy = evaluate_rfc(best_random)\n",
    "#print(random_accuracy)\n",
    "#Commented out: BECAUSE IT IS SLOWING DOWN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed03db6",
   "metadata": {},
   "source": [
    "###### Grid Search with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c784f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_search.fit(X, y)\n",
    "#grid_search.best_params_\n",
    "\n",
    "#best_grid = grid_search.best_estimator_\n",
    "#grid_accuracy = evaluate_rfc(best_grid)\n",
    "#print(grid_accuracy) \n",
    "#Commented out: BECAUSE IT IS SLOWING DOWN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3976e9",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>MODEL_REBUILDING(ANALYZING_AND_EVALUATION)</center></h1>\n",
    "\n",
    "# 7. Rebuilding model ( analyzing and evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168bf96f",
   "metadata": {},
   "source": [
    "## Model Optimization\n",
    "In machine learning, optimizing a model means finding the best set of hyperparameters for a particular problem.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "<ul>\n",
    "<li>Model hyperparameters are best thought of as settings for a machine learning algorithm that are tuned by the data scientist before training. Examples would be the number of trees in the random forest, or the number of neighbors used in K Nearest Neighbors Regression.\n",
    "</li>\n",
    "<li>Model parameters are what the model learns during training, such as the weights in the linear regression.\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "We as data scientists control a model by choosing the hyperparameters, and these choices can have a significant effect on the final performance of the model (although usually not as great of an effect as getting more data or engineering features).\n",
    "<br/>\n",
    "<i>Tuning the model hyperparameters</i> controls the balance of under vs over fitting in a model. We can try to correct for under-fitting by making a more complex model, such as using more trees in a random forest or more layers in a deep neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351c964",
   "metadata": {},
   "source": [
    "#### Hyperparameter using StandardScaler and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3dd147",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_slc = StandardScaler()\n",
    "pca = decomposition.PCA()\n",
    "logistic_Reg = lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline will helps us by passing modules one by one through GridSearchCV for which we want to get the best parameters.\n",
    "pipe = Pipeline(steps=[('std_slc', std_slc),\n",
    "                       ('pca', pca),\n",
    "                       ('logistic_Reg', logistic_Reg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e30355",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = list(range(1,X.shape[1]+1,1))\n",
    "C = np.logspace(-4, 4, 50)\n",
    "penalty = ['l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(pca__n_components=n_components,\n",
    "                  logistic_Reg__C=C,\n",
    "                  logistic_Reg__penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffcd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_GS = GridSearchCV(pipe, parameters)\n",
    "clf_GS.fit(X, y)\n",
    "\n",
    "print('Best Penalty:', clf_GS.best_estimator_.get_params()['logistic_Reg__penalty'])\n",
    "print('Best C:', clf_GS.best_estimator_.get_params()['logistic_Reg__C'])\n",
    "print('Best Number Of Components:', clf_GS.best_estimator_.get_params()['pca__n_components'])\n",
    "print(); \n",
    "print(clf_GS.best_estimator_.get_params()['logistic_Reg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84edf4f",
   "metadata": {},
   "source": [
    "#### Hyperparameter using GridSearchCV\n",
    "Before using GridSearchCV, lets have a look on the important parameters.\n",
    "<ul>\n",
    "    <li>estimator: In this we have to pass the models or functions on which we want to use GridSearchCV\n",
    "    </li>\n",
    "    <li>param_grid: Dictionary or list of parameters of models or function in which GridSearchCV have to select the best.\n",
    "    </li>\n",
    "    </ul>\n",
    "\n",
    "\n",
    "Scoring: It is used as a evaluating metric for the model performance to decide the best hyperparameters, if not especified then it uses estimator score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2fa22",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb6bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    X,\n",
    "    y,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "    cv=None,\n",
    "    n_jobs=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "):\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        return_times=True,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "title = \"Learning Curves (Naive Bayes)\"\n",
    "# Cross validation with 50 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = GaussianNB()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "title = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "estimator = svm# svm type of SVC(C = 1000, gamma = 0.1)\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89a9b8",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>CONFUSION_MATRIX</center></h1>\n",
    "\n",
    "\n",
    "# 8. Confusion Matirx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb29139",
   "metadata": {},
   "source": [
    "### Compute confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, log_reg_model_pred)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "cnf_matrix\n",
    "\n",
    "print(classification_report(y_test,log_reg_model_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b2677",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a>\n",
    "<h1 style='background:#92b0ac; border:0; color:black'><center>CONCLUSION</center></h1>\n",
    "\n",
    "\n",
    "# 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468845c9",
   "metadata": {},
   "source": [
    "### Logisitc Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d9449c",
   "metadata": {},
   "source": [
    "Hypothesis: h<sub>Θ</sub>(x)=g(Θ<sup>T</sup>*x) , h<sub>Θ</sub>(x)=g(Θ<sub>0</sub>*x<sub>0</sub>+ Θ<sub>1</sub>*x<sub>1</sub>+ ... + Θ<sub>n</sub>*x<sub>n</sub>) , g(z)= 1/(1- e<sup>-z</sup>)\n",
    "<br/> \n",
    "=> h<sub>Θ</sub>(x)=1/(1- e<sup>Θ<sub>0</sub>*x<sub>0</sub>+ Θ<sub>1</sub>*x<sub>1</sub>+ ... + Θ<sub>n</sub>*x<sub>n</sub></sup>)\n",
    "\n",
    "Suppose predict “y=1\" for h<sub>Θ</sub>(x)>=1/2 or \"y=0\" otherwise\n",
    "\n",
    "Logisitc regression cost function: J(Θ) = 1/(m) Σ {Cost (h<sub>Θ</sub>(x), y<sup>i</sup>)}=-1/m Σ{y<sup>i</sup>*log(h<sub>Θ</sub>(x))+(1-y<sup>i</sup>)log(1- h<sub>Θ</sub>(x))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c02fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4099e301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
